{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/effepivi/gvxr-tutorials/blob/main/CT_acquisition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFoh4WfDOLqi"
      },
      "source": [
        "# Session 6: CT Acquisition\n",
        "\n",
        "## CT Acquisition ![gVXR](https://github.com/effepivi/gvxr-tutorials/blob/img/gvxr_logo.png?raw=1)\n",
        "\n",
        "## Authors: Iwan Mitchell, Franck Vidal\n",
        "\n",
        "(version 1.1, 26 May 2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69QLr8RPOLql"
      },
      "source": [
        "## Aims of this Session\n",
        "- Learn how to perform a simulated CT Acquisition\n",
        "- Use reconstructed data to see how simulation parameters affect quality\n",
        "- Discover the effects of polychromatic beams on results\n",
        "- Compare simulated samples to real data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from notebook import notebookapp\n",
        "servers = list(notebookapp.list_running_servers())\n",
        "\n",
        "if len(servers) > 0:\n",
        "    if servers[0][\"notebook_dir\"] == '/':\n",
        "        print(\"This notebook is executed using Google Colab\")\n",
        "\n",
        "        !pip install -q condacolab\n",
        "        import condacolab\n",
        "        condacolab.install()"
      ],
      "metadata": {
        "id": "Q2YvKpgGOQM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from notebook import notebookapp\n",
        "servers = list(notebookapp.list_running_servers())\n",
        "\n",
        "is_running_on_Google_Colab = False\n",
        "\n",
        "if len(servers) > 0:\n",
        "    if servers[0][\"notebook_dir\"] == '/':\n",
        "        print(\"This notebook is executed using Google Colab\")\n",
        "\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()        \n",
        "\n",
        "        !pip install -q ipympl\n",
        "\n",
        "        !pip install -q k3d\n",
        "        !jupyter nbextension install --py --user k3d\n",
        "        !jupyter nbextension enable --py --user k3d\n",
        "        \n",
        "        is_running_on_Google_Colab = True\n",
        "    else:\n",
        "        print(\"This notebook is executed using Code Ocean\")"
      ],
      "metadata": {
        "id": "lsK6UzlDkfx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_running_on_Google_Colab:\n",
        "    !mamba install -c conda-forge -c intel -c astra-toolbox -c ccpi cil numpy astra-toolbox --quiet"
      ],
      "metadata": {
        "id": "pVbLL9noOwqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --quiet --upgrade gvxr xpecgen"
      ],
      "metadata": {
        "id": "SuPKZewZObPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# The directory does not exist\n",
        "if not os.path.exists(\"input_data\"):\n",
        "    \n",
        "    # Create the directory\n",
        "    os.mkdir(\"input_data\")\n",
        "\n",
        "# The directory does not exist\n",
        "if not os.path.exists(\"input_data/phantoms\"):\n",
        "    \n",
        "    # Create the directory\n",
        "    os.mkdir(\"input_data/phantoms\")\n",
        "\n",
        "# # The directory does not exist\n",
        "# if not os.path.exists(\"input_data/phantoms/spheres\"):\n",
        "    \n",
        "#     # Create the directory\n",
        "#     os.mkdir(\"input_data/phantoms/spheres\")\n",
        "\n",
        "files = [\"utilities.py\",\n",
        "         \"input_data/phantoms/doga-plate.stl\"\n",
        "        ]\n",
        "\n",
        "for fname in files:\n",
        "    if True: #not os.path.exists(fname):\n",
        "        import urllib.request\n",
        "        url = \"https://github.com/effepivi/gvxr-tutorials/raw/main/\" + fname\n",
        "        print(\"Download the file (%s) from %s\" % (fname, url))\n",
        "        urllib.request.urlretrieve(url, fname)"
      ],
      "metadata": {
        "id": "jT79FCaMOiPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubfooo5VOLqn"
      },
      "outputs": [],
      "source": [
        "import k3d\n",
        "from gvxrPython3 import gvxr\n",
        "from gvxrPython3.utils import visualise\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from utilities import recon_parallel, recon_widget, recon_cone\n",
        "import cil\n",
        "import ipywidgets as widgets\n",
        "from skimage.util import compare_images\n",
        "import tifffile as tf\n",
        "import matplotlib.cm as cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNQt6fQJOLqo"
      },
      "source": [
        "# Performing a CT Acquisition\n",
        "CT scanners are able to create 3D slices through an object by using multiple X-ray images (also known as projections, or radiographs) taken at various angles around a sample. The full details and terminology of how this reconstruction method works will be covered in tomorrow's training with CIL, for now we will treat it as a black-box function. The quality of projections directly impact the details in the final result, in this session we will take a look at what can effect our scan. *Some of the impacts on getting good scans are not obvious!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkU2puRiOLqo"
      },
      "source": [
        "## Two Case Studies\n",
        "\n",
        "To better demonstrate what is going on, we will show two examples based on real-world use cases.\n",
        "\n",
        "One use case is using an aluminium plate in a standard industrial/Lab CT machine, where we want results to measure hole diameters cut into the plate.\n",
        "\n",
        "The second case study is looking at a parallel synchrotron beam to scan tungsten fibers, and seeing how non-obvious beam properties can have a large impact on results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkjXdX3yOLqp"
      },
      "source": [
        "---\n",
        "\n",
        "# Aluminium Plate Case Study\n",
        "\n",
        "In This case study you will learn:\n",
        "- How to simulate a CT scan with gvxr using python\n",
        "- How detector properties affect result quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcZOK5yeOLqq"
      },
      "source": [
        "The aluminium plate phantom is a 50mmx50mm plate with cylindrical holes cut into the surface. The holes are in a total of 5 different sizes and do not cut through the entire plate. We would like to measure the diameters of these holes using a lab CT machine, but first we want to know if results are possible in the best-case scenario, hence using simulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ase7eChiOLqq"
      },
      "outputs": [],
      "source": [
        "# Load stl and display\n",
        "plot = k3d.plot()\n",
        "with open(\"input_data/phantoms/doga-plate.stl\", \"rb\") as model:\n",
        "    plot += k3d.stl(model.read(),color=0xfdea4f)\n",
        "plot.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr2gNbqzOLqr"
      },
      "source": [
        "## Task 1: Setup\n",
        "\n",
        "With your new-found knowledge of gvxr, setup the experiment environment as follows:\n",
        "\n",
        "1. Fixed detector 1m away from the source with a size of 216mmx36mm with a pixel pitch of 240um\n",
        "2. The detector's lsf has been measured and stored in `input_data/lsf.npy`, display and apply it to the detector\n",
        "3. A tungsten tube source with the power at 10keV\n",
        "4. The plate model `input_data/phantoms/doga-plate.stl` is in mm and should be made from aluminium, lying face-up and in focus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnCRIOXcOLqr"
      },
      "source": [
        "Notes for compatibility with CIL:\n",
        "- Ensure the detector's up vector is `[0, 0, 1]`\n",
        "- The source position is `positive y`\n",
        "- The detector position is `negative y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg6JbluJOLqs"
      },
      "outputs": [],
      "source": [
        "print(\"Create an OpenGL context\")\n",
        "\n",
        "window_id = 0\n",
        "opengl_major_version = 4\n",
        "opengl_minor_version = 5\n",
        "\n",
        "# gvxr.createOpenGLContext(window_id, opengl_major_version, opengl_minor_version);\n",
        "\n",
        "# backend = \"OPENGL\"\n",
        "# visible = True\n",
        "\n",
        "# gvxr.createWindow(window_id, visible, backend, opengl_major_version, opengl_minor_version);\n",
        "\n",
        "visible = False\n",
        "# gvxr.createWindow(window_id, visible, backend, opengl_major_version, opengl_minor_version);\n",
        "\n",
        "backend = \"EGL\"\n",
        "# visible has no effect with EGL\n",
        "gvxr.createWindow(window_id, visible, backend, opengl_major_version, opengl_minor_version);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgqsMXYqOLqs"
      },
      "outputs": [],
      "source": [
        "# Setup Detector\n",
        "gvxr.setDetectorPosition(0, -0.2, 0, \"m\")\n",
        "gvxr.setDetectorNumberOfPixels(1216, 200)\n",
        "gvxr.setDetectorPixelSize(180, 180, \"um\")\n",
        "gvxr.setDetectorUpVector(0, 0, 1)\n",
        "# gvxr.setLSF()\n",
        "\n",
        "# Setup Source\n",
        "gvxr.setSourcePosition(0,0.8,0,\"m\")\n",
        "gvxr.usePointSource()\n",
        "# gvxr.setFocalSpot(0,0,20,0.02,\"mm\",3)\n",
        "gvxr.addEnergyBinToSpectrum(10, \"keV\", 1)\n",
        "\n",
        "# Setup Plate Model\n",
        "gvxr.removePolygonMeshesFromSceneGraph()\n",
        "gvxr.loadMeshFile(\"plate\", \"input_data/phantoms/doga-plate.stl\", \"mm\")\n",
        "gvxr.setElement(\"plate\", \"Al\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOYVIuJlOLqt"
      },
      "outputs": [],
      "source": [
        "# Preview the environment\n",
        "gvxr.displayScene()\n",
        "plt.imshow(gvxr.takeScreenshot())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot=visualise(use_log=True)\n",
        "plot.display()"
      ],
      "metadata": {
        "id": "yZUZb1YzkS0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjH2hXszOLqt"
      },
      "source": [
        "## Task 2: Scanning\n",
        "Now that the scene is setup, we'll take a look into reproducing a virtual scan of the plate, by taking x-ray images and rotating the sample with `gvxr.rotateScene()`\n",
        "\n",
        "Complete the code snippets below by replacing `???` with the correct code. At the end, you'll have completed your own CT acquisition, which we'll use for reconstruction.\n",
        "We want to rotate around the detector's up axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F90XeKHOLqu"
      },
      "outputs": [],
      "source": [
        "# Define the number of projections, along with the angle step\n",
        "total_projections = 100\n",
        "final_angle = 180\n",
        "angle_step = final_angle / total_projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdpbX14NOLqu"
      },
      "outputs": [],
      "source": [
        "# Pre-create our results array with the size of our detector\n",
        "projections = np.ndarray((total_projections, 200, 1216))\n",
        "\n",
        "# Rotate our object by angle_step for every projection, saving it to the\n",
        "# results array.\n",
        "for i in range(0, total_projections):\n",
        "    # Save current angular projection\n",
        "    projections[i] = gvxr.computeXRayImage()\n",
        "    # Rotate models\n",
        "    gvxr.rotateScene(angle_step,0,0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBCEJzrxOLqv"
      },
      "outputs": [],
      "source": [
        "# Don't forget, we need to use flatfield normalisation on the radiographs to get the correct attenuation values!\n",
        "# Because our flatfield and darkfield are perfect, all we need to do is divide by the total energy\n",
        "projections /= gvxr.getTotalEnergyWithDetectorResponse()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn0GAKH0OLqv"
      },
      "source": [
        "We have now taken a full radial scan of our target object. Using these projections we can reconstruct our data into tomographical slices, and see if it's possible to measure the size of our plate's circles.\n",
        "\n",
        "There are two provided methods: `reconstruct_parallel` and `reconstruct_cone`. A more in-depth look into CIL will be in tomorrow's training. For now, treat these functions as blackboxes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "has_cil = True\n",
        "try:\n",
        "    from cil.framework import AcquisitionData, AcquisitionGeometry, ImageData\n",
        "    from cil.processors import TransmissionAbsorptionConverter\n",
        "    from cil.recon import FBP, FDK\n",
        "    #from cil.plugins.astra.processors import FBP\n",
        "    from cil.utilities.display import show_geometry\n",
        "except:\n",
        "    has_cil = False\n",
        "\n",
        "print(has_cil)"
      ],
      "metadata": {
        "id": "I1yKsYJzofYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOTlgBcCOLqv"
      },
      "outputs": [],
      "source": [
        "# Reconstruct the simulated projections with CIL.\n",
        "recon = recon_cone(projections, 180/1000, final_angle,gvxr.getSourcePosition(\"mm\"), gvxr.getDetectorPosition(\"mm\"))\n",
        "assert recon is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9PQ52KiOLqv"
      },
      "outputs": [],
      "source": [
        "# Interactive simple data display\n",
        "display(recon_widget(projections, recon))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m36xyhN_OLqw"
      },
      "source": [
        "## Task 3: Tweaking\n",
        "🎉 Congratulations, you've done a virtual CT scan!\n",
        "\n",
        "Now take a look at your results and see how well you did!\n",
        "\n",
        "With our new CT setup, we can explore what our lab scanner can do. Take a go at the little questions below and see if you can answer them!\n",
        "\n",
        "1. Does your plate clip the edge of your scanner?\n",
        "    - How can you fix this without changing the detector's size?\n",
        "2. Are the holes visible in the reconstruction?\n",
        "    - Try changing the color range to increase the contrast.\n",
        "    - Does changing the source power make your results better?\n",
        "3. What other values can you change to get a higher quality scan with the lab setup?\n",
        "    - Do more projections help?\n",
        "    - What can't you change with the lab setup that will help the CT scan?\n",
        "4. In your opinion, would this experiment be feasible on the lab setup?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txwdf2qSOLqw"
      },
      "source": [
        "---\n",
        "\n",
        "# Titanium Fibre Case Study\n",
        "\n",
        "In this case study you will learn:\n",
        "- How to automate this simulation process with json2gvxr\n",
        "- An easy way to compare real results with simulated results\n",
        "- Show how 'almost' pure sources will change real results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylMWlNZsOLqw"
      },
      "source": [
        "Our sample is a Ti/SiC composite to be scanned at a synchrotron source. It is composed of an outer titanium-aluminium-vanadium alloy with nested silicon carbide fibres housing tungsten cores. A mockup using carbon cores has also been made. However, when scanning the tungsten-cored sample, it presents artifacts not seen in the carbon-core sample.\n",
        "\n",
        "- We will take a look at how \"almost pure\" beam sources can effect scanning results with higher attenuating materials, and how these properties can be recreated with simulations.\n",
        "- A faster method of representing experimental setups and running simulated \n",
        "scans will also be introduced.\n",
        "\n",
        "\n",
        "| Sample in holder | Sample Under Microscope | \n",
        "| --- | --- |\n",
        "| ![](https://github.com/effepivi/gvxr-tutorials/blob/img/sample-annotated.png?raw=1) | ![](https://github.com/effepivi/gvxr-tutorials/blob/img/sample-microscope.png?raw=1) |\n",
        "\n",
        "![](https://github.com/effepivi/gvxr-tutorials/blob/img/CT_ref_annotated.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOtdu88lOLqx"
      },
      "source": [
        "## Task 1: Json2Gvxr\n",
        "\n",
        "So far, you have been shown how to use gvxr command-by-command. This can be a\n",
        "bit tedious when wanting a simple scanning setup. Gvxr provides utility methods\n",
        "to make life easier...\n",
        "\n",
        "`Json2gvxr` is a utility library to dynamically load and setup gvxr from a json file.\n",
        "A json file is a simple formatted text file commonly used in many different languages.\n",
        "\n",
        "Here is a snippet from `./JSON/notebook4-focal_spot.json` describing detector parameters:\n",
        "```json\n",
        "\"Detector\": {\n",
        "    \"Position\": [0, 10, 0, \"cm\"],\n",
        "    \"UpVector\": [0, 0, 1],\n",
        "    \"NumberOfPixels\": [300, 320],\n",
        "    \"Spacing\": [0.5, 0.5, \"mm\"],\n",
        "    \"LSF\": [0.00110698, 0.00122599, ...]\n",
        "},\n",
        "```\n",
        "\n",
        "Json uses `\"keys\"` to define properties, and these can be\n",
        "- objects (`{\"key\":value}`)\n",
        "- strings, (`\"string\"`)\n",
        "- numbers (`1`)\n",
        "- or arrays (`[1,\"string\", 4, {\"key\":value}, [1,2]]`)\n",
        "\n",
        "**These keys are case sensitive.**\n",
        "\n",
        "For this task you will make your own JSON config, which we'll then run using `json2gvxr`. Open up `06-Fibre.json` and follow along with the steps below for each section to create our scanning experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCqUxXOaOLqx"
      },
      "source": [
        "### Source \n",
        "\n",
        "First, we need to add a `Source` section to our file. We do this by adding `\"Source\": {}` in-between the existing two brackets in the file. This is creating a new `key` called `Source`, and giving it a value of an empty object `{}`.\n",
        "\n",
        "Your file should now look like this:\n",
        "```json\n",
        "{\n",
        "    \"WindowSize\": [800, 450],\n",
        "    \"Source\": {}\n",
        "}\n",
        "```\n",
        "\n",
        "Now we have our source object, we can setup our beam properties.\n",
        "The json format takes exactly the same parameters as the gvxr functions you've been using today.\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"Source\": {\n",
        "        \"Position\": [x, y, z, \"cm\"], // Detector position\n",
        "        \"Shape\": \"Parallel\" or \"Point\" // Beam source type\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "An important part of the source is the `\"Beam\"` key. `\"Beam\"` can take two different types of input:\n",
        "1. A list of energies\n",
        "\n",
        "    Here we have a polychromatic beam of 90% 60keV and 10% 120KeV\n",
        "    ```json\n",
        "    \"Beam\": [\n",
        "        {\n",
        "            \"Energy\": 60,\n",
        "            \"Unit\": \"keV\",\n",
        "            \"PhotonCount\": 9\n",
        "        }, \n",
        "        {\n",
        "            \"Energy\": 120,\n",
        "            \"Unit\": \"keV\",\n",
        "            \"PhotonCount\": 1\n",
        "        }, \n",
        "    ]\n",
        "    ```\n",
        "2. Parameters for an x-ray tube\n",
        "\n",
        "    Here we have an x-ray tube at 100kV, with a 5mm copper filter\n",
        "    ```json\n",
        "    \"Beam\": {\n",
        "        \"kvp\": 100,\n",
        "        \"tube angle\": 12,\n",
        "        \"filters\": [\n",
        "            [\"Cu\", 5]\n",
        "        ]\n",
        "    }\n",
        "    ```\n",
        "\n",
        "Keep an eye on your brackets! We use an array `[]` when defining each energy, but an object `{}` when defining an x-ray tube.\n",
        "\n",
        "📋 For this part of your task, make a Source entry with the following:\n",
        "- Source position of `144.92m` in x axis\n",
        "- Source type of `Parallel`\n",
        "- Beam composition of `100% 33KeV`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bMS9FnCOLqy"
      },
      "source": [
        "### Detector\n",
        "\n",
        "Now with the source defined, we can move onto creating our detector.\n",
        "Exactly the same with Source, we want to make a `\"Detector\"` key:\n",
        "*(don't forget the comma between the keys!)*\n",
        "```json\n",
        "{\n",
        "    \"Source\": {\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "    },\n",
        "    \"Detector\": {}\n",
        "}\n",
        "```\n",
        "\n",
        "Just like Source, we can setup our detector properties just like using gvxr's python functions:\n",
        "```json\n",
        "\"Detector\": {\n",
        "    \"Position\": [x, y, z, \"cm\"], // Detector position\n",
        "    \"UpVector\": [0, 0, 1], // Detector's up vector\n",
        "    \"NumberOfPixels\": [width, height], // Height and width of the detector in pixels\n",
        "    \"Spacing\": [width, height, \"mm\"], // Pixel size \n",
        "    \"LSF\": [0.00110698, 0.00122599, ...] // (Optional) Line Spread Function\n",
        "},\n",
        "```\n",
        "\n",
        "📋 For this part of the task, add to the file a Detector entry with the following:\n",
        "- Detector position of `-0.08m` in x axis\n",
        "- Up vector of `[0, 0, 1]`\n",
        "- Resolution of `1024x1024`\n",
        "- Pixel size of `1.9um`\n",
        "- LSF from `input_data/fibres/LSF.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO0tFVkWOLqy"
      },
      "source": [
        "### Samples\n",
        "\n",
        "Unlike the `Source` and `Detector` keys, `Samples` is a *list* of sample objects.\n",
        "\n",
        "Add the `Samples` key to your file, keeping in mind to use square brackets `[]` rather than curly braces.\n",
        "\n",
        "`Samples` may contain any number of samples, and can be quite complex.\n",
        "\n",
        "```json\n",
        "\"Samples\": [\n",
        "    {\n",
        "        \"Label\": \"Star\", // Label of the sample\n",
        "        \"Path\": \"./input_data/phantoms/star.stl\", // Path to the stl model file\n",
        "        \"Unit\": \"mm\", // Unit of measurements the model is using\n",
        "        \"Material\": [\"Compound\", \"SiC\"], // Material of the sample, supports \"Element\", \"Compound\", and \"Mixture\"\n",
        "        \"Density\": 3.2, // Density of the material in g/cm-3. Not required for elements.\n",
        "    },\n",
        "    {\n",
        "        \"Label\": \"Plate\",\n",
        "        \"Path\": \"./input_data/phantoms/star-plate.stl\",\n",
        "        \"Unit\": \"mm\",\n",
        "        \"Material\": [\"Mixture\", \"Zn80Sn17Fe2Cr1\"] // Mixture given as a sequence of element symbol & relative weights\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "📋 For this task, add these samples:\n",
        "\n",
        "1. Exterior matrix\n",
        "    - Path: `input_data/fibres/final_Ti90Al6V4_matrix.stl`\n",
        "    - Mixture: Ti90Al6V4\n",
        "    - Density: 4.42g/cm-3\n",
        "2. Fibres\n",
        "    - Path: `input_data/fibres/final_SiC_fibres.stl`\n",
        "    - Compound: `SiC`\n",
        "    - Density: 3.2g/cm-3\n",
        "3. Cores\n",
        "    - Path: `input_data/fibres/final_W_cores.stl`\n",
        "    - Element: `W`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHmm6-2zOLqy"
      },
      "source": [
        "### Configuring from JSON\n",
        "\n",
        "Now that we've created our configuration file, we can let json2gvxr do all the work for us in setting up our scanning environment.\n",
        "\n",
        "If you get an error, don't worry! There are easy mistakes to make:\n",
        "- Have you forgot a specific key?\n",
        "- Is the capitalisation correct?\n",
        "- Do all keys have commas after them?\n",
        "- Are all the brackets closed correctly?\n",
        "\n",
        "See what the error message tells you, fix the problem, and try again!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ObFCgsCOLqz"
      },
      "outputs": [],
      "source": [
        "from gvxrPython3 import json2gvxr\n",
        "\n",
        "# Initialise GVXR using our JSON file\n",
        "json2gvxr.initGVXR(\"./06-Fibre.json\", \"EGL\")\n",
        "\n",
        "# Load our source properties\n",
        "json2gvxr.initSourceGeometry()\n",
        "json2gvxr.initSpectrum(verbose=1)\n",
        "\n",
        "# Load our detector\n",
        "json2gvxr.initDetector()\n",
        "\n",
        "# Load our samples\n",
        "json2gvxr.initSamples(verbose=1)\n",
        "\n",
        "# Let's get an x-ray image\n",
        "fig = plt.figure()\n",
        "plt.imshow(gvxr.computeXRayImage(),cmap=\"gray\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVIGmEqNOLqz"
      },
      "source": [
        "We should now have a clear radiograph of our multi-material sample. However, it's not oriented in the correct position! With json2gvxr, we can also apply simple transformations to our samples.\n",
        "\n",
        "For every sample, add a `Transform` key and rotate 90 degrees on the x axis.\n",
        "```json\n",
        "\"Samples\": [\n",
        "    {\n",
        "        ...\n",
        "        ...\n",
        "        \"Transform\": [\n",
        "            [\"Rotation\", degrees, x, y, z]\n",
        "        ]\n",
        "\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "If you've managed to succeed in the task so far, you'll end up with an image like the one below:\n",
        "\n",
        "![](https://github.com/effepivi/gvxr-tutorials/blob/img/CT-task1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRIABCXMOLqz"
      },
      "source": [
        "### Scanning & Reconstructing\n",
        "\n",
        "Scanning objects with high resolution detectors produce a very large amount of data. This also applies to simulated scans as well! In order to save some memory space, we can consider obtaining enough data just for one reconstructed slice of our sample.\n",
        "\n",
        "If we were to simulate and reconstruct with out current setup, with 700 projections we would use: \n",
        "$$700\\times1024\\times1024\\times32 \\text{~bytes} = 23488.10 \\text{~MB}$$\n",
        "\n",
        "That is a large amount of space just for the projections! This is not mentioning the additional amount needed to perform the tomographical reconstruction.\n",
        "\n",
        "One method covered in the detector notebook would be to artificially increase the pixel size, but this would result in a coarser image. In our specific case, we can use the sample's vertical uniformity to our advantage and only take a central slice with a one pixel high detector. This decreases our memory usage drastically:\n",
        "$$700 \\times 1024 \\times 1 \\times 32 \\text{~bytes} = 22.94 \\text{~MB}$$\n",
        "\n",
        "Go back and change your json file to a 1-pixel tall detector, then continue onwards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA5Ja6u9OLqz"
      },
      "source": [
        "Json2gvxr can also handle scanning automatically for you. However, we're not going to cover that feature in this notebook. Feel free to take a look at the `scripts/pump-json-scan.py` and `JSON/Turbopump.json` example to leverage that functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkaO6P20OLqz"
      },
      "outputs": [],
      "source": [
        "# Ensure detector's height is 1 pixel tall\n",
        "assert gvxr.getDetectorNumberOfPixels()[1] == 3\n",
        "\n",
        "# Define the number of projections, along with the angle step\n",
        "total_projections = 700\n",
        "final_angle = 180\n",
        "angle_step = final_angle / total_projections\n",
        "\n",
        "# Pre-create our results array with the size of our detector\n",
        "projections = np.ndarray((total_projections,3, 1024))\n",
        "\n",
        "# Rotate our object by angle_step for every projection, saving it to the\n",
        "# results array.\n",
        "for i in range(0, total_projections):\n",
        "    # Save current angular projection\n",
        "    projections[i] = np.asarray(gvxr.computeXRayImage())\n",
        "    # Rotate models\n",
        "    gvxr.rotateScene(angle_step,0,0,1)\n",
        "\n",
        "# Don't forget to apply flatfield!\n",
        "projections /= gvxr.getTotalEnergyWithDetectorResponse()\n",
        "recon = recon_parallel(projections, 1.9/1000, final_angle) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2stTff5-OLqz"
      },
      "outputs": [],
      "source": [
        "plt.imshow(recon[1],cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6R5X3WlOLq0"
      },
      "source": [
        "## Task 2: Comparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrQwLHb7OLq0"
      },
      "source": [
        "It can be quite hard to compare data with just large image figures. Focusing on a region of interest and using techniques such as checkerboard patterns make comparing two images easier.\n",
        "\n",
        "Empirical testing is also possible, with image comparison functions such as absolute differences and [cross correlation methods](https://en.wikipedia.org/wiki/Cross-correlation). Here we will use both checkerboards and ZNCC - Zero-Normalised Cross Correlation, to rank the similarity between two images, and display an easy to understand comparison between two images.\n",
        "\n",
        "We will be comparing ground truth data to our simulated scan to better identify the artifacts caused by our scanning setup. Using scikit learn's `compare_images` function, we are able to create a checkerboard image and display with matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDl3KVj0OLq0"
      },
      "outputs": [],
      "source": [
        "# Load and reconstruct reference CT\n",
        "reference_projections = np.asarray(tf.imread(\"../input_data/fibres/reference_normalised_projections.tif\"))\n",
        "reference_projections = reference_projections.reshape((900,1,1024))\n",
        "reference_projections = np.hstack((reference_projections,reference_projections))\n",
        "reference_CT = recon_parallel(reference_projections,1.9/1000,180)[0]\n",
        "\n",
        "# Create a checkerboard image\n",
        "comp = compare_images(reference_CT, recon[1],\"checkerboard\",n_tiles=(8,8))\n",
        "\n",
        "# Display images in a 1x3 grid\n",
        "norm = cm.colors.Normalize(vmax=2, vmin=-1)\n",
        "fig, axes = plt.subplots(1,3)\n",
        "axes[0].imshow(reference_CT, cmap=\"gray\", norm=norm)\n",
        "axes[1].imshow(recon[1], cmap=\"gray\", norm=norm)\n",
        "axes[2].imshow(comp, cmap=\"gray\", norm=norm)\n",
        "axes[0].set_axis_off()\n",
        "axes[1].set_axis_off()\n",
        "axes[2].set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvitRTPsOLq0"
      },
      "source": [
        "In the cell above, we can see our reconstructed data and reference side-by-side. However, they are not correctly oriented.\n",
        "\n",
        "📋 Go back to the scanning json file and add another Rotation entry to the samples to get a similar result as the one below:\n",
        "\n",
        "![](https://github.com/effepivi/gvxr-tutorials/blob/img/CT-task2-orient.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCDRHks-OLq1"
      },
      "source": [
        "### Region of interest\n",
        "\n",
        "We are interested on scanning artifacts centered around the cores in the sample. With the current size of the plots comparing cores is quite difficult.\n",
        "Your task is to crop the images using numpy slicing to target a region of interest covering a few cores with streaking artifacts.\n",
        "\n",
        "```python\n",
        "ref_zoom = reference_CT[200:300,200:300]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOiRXM6WOLq1"
      },
      "outputs": [],
      "source": [
        "ref_roi = reference_CT[200:500,200:500]\n",
        "recon_roi = recon[1][200:500,200:500]\n",
        "\n",
        "# Create a checkerboard image\n",
        "comp = compare_images(ref_roi, recon_roi,\"checkerboard\",n_tiles=(8,8))\n",
        "\n",
        "# Display images in a 1x3 grid\n",
        "norm = cm.colors.Normalize(vmax=2, vmin=-1)\n",
        "fig, axes = plt.subplots(1,3)\n",
        "axes[0].imshow(ref_roi, cmap=\"gray\", norm=norm)\n",
        "axes[1].imshow(recon_roi, cmap=\"gray\", norm=norm)\n",
        "axes[2].imshow(comp, cmap=\"gray\", norm=norm)\n",
        "axes[0].set_axis_off()\n",
        "axes[1].set_axis_off()\n",
        "axes[2].set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik1EnhIYOLq1"
      },
      "source": [
        "### ZNCC\n",
        "\n",
        "Visual inspections are handy for at-a-glance checking of images. However we need a more empirical method to measure similarity for automated methods.\n",
        "\n",
        "Zero-Normalised Cross Correlation (ZNCC) is a method commonly used in computer vision to detect similarity regardless of contrast difference in images. Apply this method to compare the similarity "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff44hzl9OLq1"
      },
      "outputs": [],
      "source": [
        "def zncc(reference:np.ndarray, target:np.ndarray) -> float:\n",
        "    # Must first perform a zero-mean unit-variance normalisation\n",
        "    normalised_ref = (reference - reference.mean()) / reference.std()\n",
        "    normalised_target = (target - target.mean()) / target.std()\n",
        "    return 1-((1.0 - np.mean(np.multiply(normalised_ref, normalised_target))) / 2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coI8y70MOLq2"
      },
      "outputs": [],
      "source": [
        "# Create a function to display both images, along with a checkerboard image and\n",
        "# zncc score.\n",
        "def compare(reference, target):\n",
        "    norm = cm.colors.Normalize(vmax=2, vmin=-1)\n",
        "    fig, axes = plt.subplots(1,3)\n",
        "    axes[0].imshow(reference, cmap=\"gray\", norm=norm)\n",
        "    axes[0].set_title(\"Reference\")\n",
        "    axes[1].imshow(target, cmap=\"gray\", norm=norm)\n",
        "    axes[1].set_title(\"Target\")\n",
        "    axes[2].imshow(comp, cmap=\"gray\", norm=norm)\n",
        "    \n",
        "    axes[2].set_title(f\"ZNCC: {zncc(reference, target):.2%}\")\n",
        "\n",
        "    axes[0].set_axis_off()\n",
        "    axes[1].set_axis_off()\n",
        "    axes[2].set_axis_off()\n",
        "    plt.plot()\n",
        "\n",
        "compare(ref_roi, recon_roi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRu8sRGAOLq3"
      },
      "source": [
        "## Task 3: Polychromatism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaDPLnMaOLq3"
      },
      "source": [
        "The last part of this case study is discovering what is causing the dark streaks between cores. As mentioned in the task introduction, this artefact only appears when using tungsten cores and is different from the vertical white streaks we get from normal tungsten behavior.\n",
        "\n",
        "As you may've guessed, these streaks are caused by the \"almost\" pure beam the original experiment was using. The beam was a 97% pure beam at 33keV, however it contained 2% of 66keV and 1% at 99keV.\n",
        "\n",
        "📋 For your final task:\n",
        "1. Add in the 3% of beam harmonics and compare results.\n",
        "2. Perform another scan using carbon cores with the same beam parameters.\n",
        "3. Which power is causing the biggest impact? \n",
        "4. Do the effects appear if using a monochromatic source at a different power?\n",
        "5. What else is missing from the simulation to get a matching image?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jxMWscBOLq3"
      },
      "source": [
        "---\n",
        "\n",
        "## Extension Task: Scanning your own model\n",
        "\n",
        "Now that you've learnt how to create CT scans using gVXR, take a go at making your own!\n",
        "\n",
        "We have included a collection of phantoms and components for your virtual scanner in `input_data/`. \n",
        "Have a go at selecting a model and performing a CT scan to create some interesting data for tomorrow's reconstruction training. \n",
        "Take a look at changing component materials, tweaking your source and detector, using transformations, and other parameters too see how powerful gVXR is!\n",
        "\n",
        "Feel free to use either gVXR's API, or the JSON approach for your scans. \n",
        "I personally favor the JSON approach and use gVXR's API when something more complex needs to be done. \n",
        "Just remember you will need your scanning parameters in order to work with the data tomorrow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOH3BCXROLq3"
      },
      "source": [
        "### Lost for ideas? Below are a couple of suggestions. \n",
        "\n",
        "We have selected two CAD models.\n",
        "\n",
        "| | Turbo pump | 2 stroke engine |\n",
        "|-|-----------|-----------------|\n",
        "| **File location** | `../input_data/TurboPump` | `../input_data/2StrokeEngine` |\n",
        "| **Number of STL files** | 6 | 5 |\n",
        "| **Preview**|  ![](https://github.com/effepivi/gvxr-tutorials/blob/img/turbo_pump-screenshot.png?raw=1) | ![](https://github.com/effepivi/gvxr-tutorials/blob/img/2stroke_engine-screenshot.png?raw=1)  |\n",
        "\n",
        "#### 1. Choose one model or the other. \n",
        "#### 2. Create your own Jupyter Notebook. \n",
        "#### 3. We suggest that you initialise gVXR with the parameters as follows:\n",
        "\n",
        "| | Turbo pump | 2 stroke engine |\n",
        "|-|-----------|-----------------|\n",
        "| **Source shape** | Point | Point |\n",
        "| **Source position** | [0, 200, 0, \"mm\"] |  [0, 600, 0, \"mm\" ]\n",
        "| **Photon energy** | 500 keV |  350 kV |\n",
        "| **Detector position** | [0, -150, 0, \"mm\"] |  [0, -150, 0, \"mm\"] |\n",
        "| **Detector up vector** | [0, 0, 1] |  [0, 0, 1] |\n",
        "| **Number of pixels** | [900, 900] | [900, 900] |\n",
        "| **Pixel spacing** | [0.2, 0.2, \"mm\"] | [0.5, 0.5, \"mm\"] |\n",
        "\n",
        "#### 4. Load each STL file of the model you chose. \n",
        "You can tweak the material properties of each component.\n",
        "Feeling lazy or need a bit of extra help? We provided a possible configuration for both models:\n",
        "\n",
        "- [../JSON/Turbopump.json](../JSON/Turbopump.json) Note that the geometry of the turbo pump is rotated by 90 degrees around the X-axis, and rescaled by a factor of 0.2 on all axes.\n",
        "- [../JSON/2StrokeEngine.json](../JSON/2StrokeEngine.json)\n",
        "\n",
        "(I know, it's a bit tidious to set the material properties of mixtures)\n",
        "\n",
        "#### 5. Once you initialised all the detector, source and sample properties, we suggest that you compute an X-ray projection then call `plotScreenshot()` or `visualise()` to check that everything is in order. \n",
        "Remember to import them as follows:\n",
        "\n",
        "```python\n",
        "from gvxrPython3.utils import visualise\n",
        "from gvxrPython3.utils import plotScreenshot\n",
        "```\n",
        "\n",
        "To use the 3D visualisation, call\n",
        "\n",
        "```python\n",
        "plot=visualise()\n",
        "plot.display()\n",
        "```\n",
        "\n",
        "#### 6. Now you should be ready to run CT scan.\n",
        "\n",
        "In the JSON files mentioned above, we compute 721 projections over 360 degrees. \n",
        "\n",
        "#### 7. Visualise radiographs and reconstruction data\n",
        "\n",
        "If you used the geometry we suggested (point source), you may use the code as follow to visualise the radiographs and reconstruct the CT volume:\n",
        "\n",
        "```python\n",
        "recon = recon_cone(projections, pixel_size, final_angle, source_position, detector_position)\n",
        "display(recon_widget(projections, recon))\n",
        "```\n",
        "\n",
        "If you were adventurous nd used a parallel geometry, you should use:\n",
        "\n",
        "```python\n",
        "recon = recon_parallel(projections, pixel_size)\n",
        "display(recon_widget(projections, recon))\n",
        "```\n",
        "\n",
        "#### 8. Want to see the solution?\n",
        "\n",
        "We provide a solution for each CAD model:\n",
        "\n",
        "- The first one makes use of the JSON approach for setting the X-ray simulation parameters, and the gVXR API for the actual CT scan acquisition. You can find it in [../JSON/pump_CT.ipynb](../JSON/pump_CT.ipynb). \n",
        "- The second one makes use of the JSON approach only. It is the less intensive one in terms of coding. You can find it in [../JSON/2StrokeEngine.ipynb](../JSON/2StrokeEngine.ipynb). \n",
        "\n",
        "\n",
        "<!-- \n",
        "Choose from this list:\n",
        "\n",
        "\n",
        "- Have a go at X-raying `TurboPump`\n",
        "- What detector dimensions do you need for scanning `2StrokeEngine`?\n",
        "- With a 96% pure beam, do impurities effect `doga-spheres.stl` made from tungsten?\n",
        "- Make a notebook to quickly scan a collection of models with the same CT setup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Lost for ideas? Choose from this list:\n",
        "- Have a go at X-raying `TurboPump`\n",
        "- What detector dimensions do you need for scanning `2StrokeEngine`?\n",
        "- With a 96% pure beam, do impurities effect `doga-spheres.stl` made from tungsten?\n",
        "- Make a notebook to quickly scan a collection of models with the same CT setup -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aKbnsvVOLq4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsLG5LkZkeRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "7cb52200ce58d0990af40dfb29272ed58ebf24dc16709b96f71cbdb887d325ae"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}